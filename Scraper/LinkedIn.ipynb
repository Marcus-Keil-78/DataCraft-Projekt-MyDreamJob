{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import der Module\n",
    "# Pyhton Version 3.11.1\n",
    "# Selenium Version 4.9.1\n",
    "# webdriver_manager Version 3.8.6\n",
    "# time\n",
    "# random\n",
    "# pandas Version  2.0.1\n",
    "# datetime Version 5.1\n",
    "# sqlalchemy Version 2.0.13\n",
    "# os \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "#----------------------------------------------------------------------------------\n",
    "## Hilfsfunktionen\n",
    "# Log File\n",
    "def schreibe_log_file(log_eintrag):\n",
    "    datei='log-LinkedIn.txt' # Name der Log-Datei\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Überprüfen, ob die Log-Datei bereits existiert\n",
    "    if not os.path.exists(datei):\n",
    "        # Wenn die Log-Datei nicht existiert, wird sie erstellt und der Log-Eintrag wird hineingeschrieben\n",
    "        with open(datei, 'w') as file:\n",
    "            file.write(f\"{timestamp}, {log_eintrag}\" + '\\n')\n",
    "    else:\n",
    "        # Wenn die Log-Datei bereits existiert, wird der Log-Eintrag an das Ende der Datei angehängt\n",
    "        with open(datei, 'a') as file:\n",
    "            file.write(f\"{timestamp}, {log_eintrag}\" + '\\n')\n",
    "            \n",
    "# E-Mail - Benachrichtigung bei Fehlern\n",
    "def schreibe_e_mail(message, Subject=\"fehlerhaft\"):\n",
    "    # SMTP-Server-Konfiguration\n",
    "    host = \"smtp.web.de\"\n",
    "    port = 587\n",
    "    log_in_id = \"\"\n",
    "    passwort = \"\"\n",
    "\n",
    "    # Erstellen der E-Mail-Nachricht\n",
    "    msg = MIMEMultipart()\n",
    "    msg['From'] = log_in_id + \"@web.de\" # Absender\n",
    "    msg['To'] = \"\" # Empfänger\n",
    "    msg['Subject'] = f'LinkedIn Abfrage {Subject}' # Betreff der E-Mail\n",
    "\n",
    "    msg.attach(MIMEText(message, 'plain')) # Hinzufügen des Nachrichtentextes zur E-Mail\n",
    "\n",
    "    # Verbindung zum SMTP-Server herstellen und E-Mail senden\n",
    "    with smtplib.SMTP(host=host, port=port) as mail:\n",
    "        mail.starttls() # Starte TLS-Verschlüsselung\n",
    "        mail.login(log_in_id, passwort) # Anmeldung am SMTP-Server\n",
    "        mail.send_message(msg) # E-Mail senden\n",
    "\n",
    "# random Wartezeit zum für seitenaufbau oder so\n",
    "def wartezeit(zeit=3):\n",
    "    \"\"\"\n",
    "    Diese Funktion fügt eine zufällige Wartezeit hinzu, bevor sie fortgesetzt wird.\n",
    "\n",
    "    :param zeit: Die maximale Wartezeit in Sekunden (Standardwert ist 3).\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    time.sleep(random.randint(1, zeit))\n",
    "\n",
    "# Da LinkedIn (zwecks Erkennung auf Bot´s) gerne eine Anmeldung möchte diese Abfangen\n",
    "def anmeldemaske_finden(driver):\n",
    "    # Anmeldemaske finden und text Abspeichern\n",
    "    try:\n",
    "        return driver.find_element(By.CLASS_NAME, 'authwall-join-form__title').text == 'Mitglied werden'\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "#---------------------------------------------------------------------------\n",
    "## SQL-Server verbindung erstellen\n",
    "# SQL-Server-Konfiguration\n",
    "#host=\"datacraft-db.cf3hyz1fwdiw.eu-central-1.rds.amazonaws.com\"\n",
    "host=\"\"\n",
    "database=\"\"\n",
    "user=\"\"\n",
    "password=\"\"\n",
    "tabelle_Rohdaten=\"jobs.rohdaten\"\n",
    "\n",
    "# Erstelle eine SQL-Verbindung mit der Datenbank\n",
    "connection = create_engine(f'postgresql+psycopg2://{user}:{password}@{host}/{database}')\n",
    "#----------------------------------------------------------------------------\n",
    "## Der Scraper\n",
    "def scraper(jobtitel, suchort='Deutschland'):\n",
    "    '''\n",
    "    Der eigentliche Scraper für die Seite\n",
    "    \n",
    "    :param jobtitel: Der Titel des Jobs, nach dem gesucht werden soll.\n",
    "    :param suchort: Der Ort, an dem nach Jobs gesucht werden soll. Standardwert ist 'Deutschland'.\n",
    "    '''\n",
    "    \n",
    "    schreibe_log_file(f'suche nach {jobtitel}')\n",
    "    \n",
    "    # ChromeOptions erstellen\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    #Headless-Modus aktivieren\n",
    "    #chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "    # Browserfenster öffnen nach Einstellung in den Optionen\n",
    "    try:\n",
    "        driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=chrome_options)\n",
    "        schreibe_log_file('Browser geoeffnet')\n",
    "    except:\n",
    "        schreibe_log_file('Browser konnte nicht geoeffnet werden')\n",
    "        schreibe_e_mail(f'Browser konnte nicht geoeffnet werden: -- {datetime.datetime.now().strftime(\"%d.%m.%Y, %H:%M:%S\")} --')\n",
    "\n",
    "    # Webseite aufrufen\n",
    "    wartezeit(1)\n",
    "    driver.get(\"https://de.linkedin.com/jobs/search?keywords=&location=&geoId=&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0\")\n",
    "\n",
    "    # Fenster Maximieren\n",
    "    driver.maximize_window() # evtl. im Headless-Modus nicht machbar\n",
    "    #--------------------------------------------------------------------------\n",
    "    ## Cookies akzeptieren\n",
    "    wartezeit(1)\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"artdeco-global-alert-container\"]/div/section/div/div[2]/button[1]').click()\n",
    "    wartezeit(1)\n",
    "    \n",
    "    if anmeldemaske_finden(driver=driver) == True:\n",
    "        schreibe_log_file(f'als Bot erkannt')\n",
    "        # Schließen des Browsers\n",
    "        wartezeit(1)\n",
    "        driver.quit()\n",
    "        schreibe_log_file('Browser geschlossen')\n",
    "    else:\n",
    "        ## Suchfelder finden, anwählen und befüllen\n",
    "        # Suchfeld Jobbezeichnung -> finden und befüllen\n",
    "        suchfeld_jobtitel=driver.find_element(By.XPATH,'//*[@id=\"job-search-bar-keywords\"]')\n",
    "        suchfeld_jobtitel.send_keys(jobtitel)#\"jobtitel\" wird an die Funktion übergeben\n",
    "        wartezeit(2)\n",
    "\n",
    "        # Suchfeld Ort -> finden und befüllen\n",
    "        suchfeld_ort=driver.find_element(By.XPATH,'//*[@id=\"job-search-bar-location\"]')\n",
    "        suchfeld_ort.clear()\n",
    "        wartezeit(2)\n",
    "        suchfeld_ort.send_keys(suchort)#\"suchort\" wird an die Funktion übergeben\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Anfrage mit Enter/Return abschicken\n",
    "        suchfeld_ort.send_keys(Keys.RETURN)\n",
    "        time.sleep(random.randint(5,10))\n",
    "\n",
    "        # Überprüfen ob der Suchort noch mit der Eingabe übereinstimmt\n",
    "        \n",
    "        try:\n",
    "            jobtitel_feld=driver.find_element(By.XPATH,'//*[@id=\"job-search-bar-keywords\"]').get_attribute('value')\n",
    "            suchort_feld=driver.find_element(By.XPATH,'//*[@id=\"job-search-bar-location\"]').get_attribute('value')\n",
    "        except:\n",
    "            schreibe_log_file(f'als Bot erkannt: {link}')\n",
    "            return\n",
    "                \n",
    "        if suchort_feld != suchort or jobtitel_feld != jobtitel:\n",
    "            schreibe_log_file(f'Der Suchort oder Jobtitel wurde von LinkedIn abgeaendert, evtl. wurden wir als BOT erkannt!!!')\n",
    "            schreibe_e_mail(f'Der Suchort oder Jobtitel wurde von LinkedIn abgeaendert, evtl. wurden wir als BOT erkannt!!!\\n\\n>>>{datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}<<<\\n\\nJobtitel: {jobtitel}\\nSuchort: {suchort}\\n\\nAutomatisch abgeändert in:\\nJobtitel: {jobtitel_feld}\\nSuchort: {suchort_feld}')\n",
    "            wartezeit(1)\n",
    "            driver.quit()\n",
    "            schreibe_log_file('Browser geschlossen')\n",
    "            #break\n",
    "        \n",
    "        else:\n",
    "\n",
    "            ## Log-In Fenster entfernen\n",
    "            driver.find_element(By.XPATH, '/html/body/div[3]/button').click()\n",
    "            time.sleep(random.randint(5,10))\n",
    "            #-------------------------------------------------------------------------\n",
    "            ## Seite für weitere Stellenanzeigen nach unten Scrollen\n",
    "            for i in range(6):\n",
    "                driver.execute_script(\"window.scrollBy(0, document.body.scrollHeight);\")\n",
    "                time.sleep(random.randint(3.0,4.0))\n",
    "            #-------------------------------------------------------------------------\n",
    "            link_liste_scraped=[]\n",
    "\n",
    "            anzeigen=driver.find_elements(By.CLASS_NAME, 'base-card__full-link')\n",
    "\n",
    "            for anzeige in anzeigen:\n",
    "                link_liste_scraped.append(anzeige.get_attribute('href'))\n",
    "            #-------------------------------------------------------------------------\n",
    "            # überprüfen ob link_liste_scraped leer ist da evtl. die Class name geändert wurde\n",
    "            if len(link_liste_scraped) == 0:\n",
    "                schreibe_log_file('Keine Links auf gefunden')\n",
    "                schreibe_e_mail('Keine Links auf Webseite gefunden:\\n\\n Hinweis: Classenname ueberpruefen fuer alle Stellenanzeigen!')\n",
    "                return\n",
    "            else:\n",
    "                pass\n",
    "            #------------------------------------------------------------------------\n",
    "            # Duplikate entfernen\n",
    "            link_liste_scraped_ohne_duplikate = list(set(link_liste_scraped))\n",
    "            schreibe_log_file(f'Es wurden {len(link_liste_scraped)-len(link_liste_scraped_ohne_duplikate)} Duplikate in der \"Link Liste\" entfernt')\n",
    "            schreibe_log_file(\"link liste erstellt\")\n",
    "            #------------------------------------------------------------------------\n",
    "            # Lese die bestehenden URLs aus der Datenbank\n",
    "            existing_urls = pd.read_sql_query(f\"SELECT url FROM {tabelle_Rohdaten}\", connection)\n",
    "            schreibe_log_file(f'{len(existing_urls)} in der datenbak bereits vorhanden')\n",
    "            \n",
    "            # Filtere den DataFrame, um nur neue URLs zu behalten\n",
    "            link_liste = [url for url in link_liste_scraped_ohne_duplikate if url not in existing_urls['url'].values]\n",
    "            #------------------------------------------------------------------------\n",
    "            seiten_inhalt_html_liste = []\n",
    "            seiten_inhalt_liste = []\n",
    "            URL_Liste = []\n",
    "            Datum_Liste = []\n",
    "\n",
    "            try:\n",
    "                schreibe_log_file('Abfrage begonnen')\n",
    "                for link in link_liste:\n",
    "                    \n",
    "                    # Seite Aufrufen\n",
    "                    driver.get(link)\n",
    "                    wartezeit()\n",
    "                    \n",
    "                    if anmeldemaske_finden(driver=driver) == False:\n",
    "                    \n",
    "                        #Angaben erweitern\n",
    "                        try:\n",
    "                            driver.find_element(By.XPATH, '//*[@id=\"main-content\"]/section[1]/div/div/section[1]/div/div/section/button[1]').click()\n",
    "                        except:\n",
    "                            pass\n",
    "                        wartezeit(2)\n",
    "                        \n",
    "                        # Listen befüllen\n",
    "                        seiten_inhalt_html_liste.append(driver.find_element(By.CLASS_NAME, 'details').get_attribute('innerHTML'))\n",
    "                        seiten_inhalt_liste.append(driver.find_element(By.CLASS_NAME, 'details').text)\n",
    "                        URL_Liste.append(link)\n",
    "                        Datum_Liste.append(datetime.datetime.now())\n",
    "                        schreibe_log_file(f'Daten wurden gezogen: {link}')\n",
    "                        time.sleep(random.randint(2,10))\n",
    "                    \n",
    "                    else:\n",
    "                        schreibe_log_file(f'als Bot erkannt: {link}')\n",
    "            except:\n",
    "                schreibe_log_file(\"Abfrage abgebrochen !!!\")\n",
    "                schreibe_e_mail(f\"Die Abfrage wurde abgebrochen. -- {datetime.datetime.now()} --\")\n",
    "            schreibe_log_file('Abfrage beendet')\n",
    "            #-------------------------------------------------------------------------\n",
    "            # Schließen des Browsers\n",
    "            wartezeit(1)\n",
    "            driver.quit()\n",
    "            schreibe_log_file('Browser geschlossen')\n",
    "            #-------------------------------------------------------------------------\n",
    "            #Überprüfen ob die Listen gleich lang sind und DataFrame erstellen\n",
    "            if len(set(map(len, [seiten_inhalt_html_liste,\n",
    "                                seiten_inhalt_liste,\n",
    "                                URL_Liste,\n",
    "                                Datum_Liste]))) > 1:\n",
    "                schreibe_log_file('Listen sind verschieden lang')\n",
    "                schreibe_e_mail('Listen sind nicht gleich lang')\n",
    "                return\n",
    "            else:\n",
    "                # DataFrame erstellen\n",
    "                df = pd.DataFrame({\"seite\": \"linkedin\",\n",
    "                                \"seiten_inhalt_html\": seiten_inhalt_html_liste,\n",
    "                                \"seiten_inhalt\": seiten_inhalt_liste, \n",
    "                                \"url\": URL_Liste, \n",
    "                                \"datum\":Datum_Liste,\n",
    "                                \"storno\": False})\n",
    "            #------------------------------------------------------------------------\n",
    "            ## Daten in die Datenbank einfügen\n",
    "            # Schreibe den bereinigten DataFrame in die Datenbank\n",
    "            df.to_sql(name=tabelle_Rohdaten.split('.')[1],\n",
    "                    schema=tabelle_Rohdaten.split('.')[0],\n",
    "                    con=connection, if_exists='append', index=False, )\n",
    "            schreibe_log_file(f'Es wurden {len(df)} Daten von LinkedIn hinzugefuegt')\n",
    "        \n",
    "        # Verbindung zur SQL-Datenbank schließen\n",
    "        connection.dispose()\n",
    "        #else:\n",
    "        #    schreibe_log_file(f'als Bot erkannt: {link}')\n",
    "print(\"fertig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ich arbeite mit zwei Computern, und leider hat die Datei für die Suchbegriffe (Jobtitel) jeweils ein anderen Speicherort\n",
    "try:\n",
    "    suchbegriffe = pd.read_excel(r\"C:\\Daten@DS-Server\\Python\\Block 9.2 - Praxisprojekt\\Jobtitel.xlsx\",\n",
    "                                sheet_name='LinkedIn')\n",
    "except:\n",
    "    suchbegriffe = pd.read_excel(r\"C:\\Daten@Server\\Python\\Block 9.2 - Praxisprojekt\\Jobtitel.xlsx\",\n",
    "                                sheet_name='LinkedIn')  \n",
    "display(suchbegriffe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(suchbegriffe)} Jobtitel gefunden')\n",
    "for i, jobtitel in enumerate(suchbegriffe.Jobtitel):\n",
    "    print(f'{i}: Suche nach \"{jobtitel}\", Start: {datetime.datetime.now().strftime(\"%H:%M:%S\")} Uhr')\n",
    "    try:\n",
    "        scraper(jobtitel)\n",
    "    except UnboundLocalError as e:\n",
    "        schreibe_e_mail(f'Scraper abgebrochen\\nJobtitel: {jobtitel}\\n    Zeit: {datetime.datetime.now()}\\n\\nFehler:\\n{e}')\n",
    "        schreibe_log_file(f'Scraper abgebrochen; Jobtitel: {jobtitel}')\n",
    "        print(f'Scraper abgebrochen; Jobtitel: {jobtitel}')\n",
    "print(\"\")\n",
    "print(\"fertig\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
